# -*- coding: utf-8 -*-
"""sdg_relevance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eQfJwW9oh-O8r42_lTOpeHRSyDm97rvh
"""

#pip install pycld2
#pip install PyPDF2
#pip install --upgrade translators

import pandas as pd
import numpy as np
import pycld2 as cld2
import translators as ts
import translators.server as tss
import pdf_handler as pdh
import text_handler as txh
import csv
import warnings
warnings.filterwarnings("ignore")


# Util functions
def get_content(input, uuid):
    if input == "":
        return ""
    output = ""
    detected_language = cld2.detect(input,  returnVectors=True)
    from_lang = detected_language[2][0][1]

    if detected_language[2][0][1] == "un":
        print("Bad PDF content. id: {}".format(uuid))
        return ""

    if detected_language[2][0][1] != "en":
        try:
            output = txh.processed_text(tss.google(input, from_lang, "en", if_ignore_empty_query=True, if_ignore_limit_of_length=True,))
        except:
            print("Error occured while translation from {0}. id: {1}".format(from_lang, uuid))
    else:
        output = txh.processed_text(input)
    return output

def write_csv_line(row, csv):
    paper_data = ""
    pdf_data = ""
    abs_data = ""

    if type(row.filepath) == str: 
        pdf_url = url_base + row.filepath
        pdf_content = txh.clean_text(pdh.get_pdf_content(pdf_url))
        pdf_data = get_content(pdf_content, row.uuid)
    if type(row.abstract) == str:
        abs_data = get_content(row.abstract, row.uuid)

    paper_data = pdf_data + " " + abs_data

    # if abs and pdf is not empty
    if (len(paper_data) > 10):
        sdg_relevances = calc_CMU(paper_data)
        #sdg_relevances = calc_CMU_WF(paper_data)
        #sdg_relevances = calc_CMU_UOA(paper_data)
        uuid = row.uuid
        pdf = len(pdf_data) > 0
        abs = len(abs_data) > 0
        csv_row = [uuid, pdf, abs] + sdg_relevances
        csv.writerow(csv_row)

# Weighted Frequency Search
def calc_CMU_WF(text):
    match_score = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
    text_len = len(text.split())
    for i in range(len(df_keywords)):
        count = text.count(df_keywords.keyword[i])
        match_score[df_keywords.goal[i] - 1] += (df_keywords.weight[i] * count)
    return match_score

# Default search, only controls occurrences
def calc_CMU(text):
    match_score = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
    text_len = len(text.split())
    for i in range(len(df_keywords)):
        if text.find(df_keywords.keyword[i]) > 0:
            match_score[df_keywords.goal[i] - 1] += 1
    return match_score

# Uses UOA dataset, more academical
def calc_UOA(text):
    match_score = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
    text_len = len(text.split())
    for i in range(len(df_keywords_UOA)):
        if text.find(df_keywords_UOA['SDG Keywords'][i]) > 0:
            match_score[int(df_keywords_UOA['SDG'][i][-1]) - 1] += 1
    return match_score

# Main
def main():
    out_file = open('output.csv', 'w') 
    output = csv.writer(out_file)
    output.writerow(headers)
    count= 0
    for index, row in df_data.iterrows():
        write_csv_line(row, output)
        count += 1
        print(count)
    out_file.close()

to_lang = "en"
url_base = "https://gcris.iyte.edu.tr/"
headers = ["uuid", "pdf", "abs", 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]

df_keywords = pd.read_csv("data/cmu1000_keywords_cleaned.csv")
df_keywords_UOA = pd.read_excel("data/UoA-SDG-Keyword-List-Ver.-1.1.xlsx")
df_data = pd.read_csv("data/gcris_view_items.csv", encoding="utf-8")
#df_keywords_UOA.head()
#df_keywords.head()
main()
